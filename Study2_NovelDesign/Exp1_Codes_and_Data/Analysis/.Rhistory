library(lme4)
library(rstudioapi)
library(rstan)
library(reshape2)
library(lm.beta)
library(lmerTest)
library(dplyr)
library(tidyr)
# library(gganimate)
rm(list=ls())
options(mc.cores = parallel::detectCores())
# Fix terrible plotting on Windows
trace(grDevices:::png, quote({if (missing(type) && missing(antialias)) {
type <- "cairo-png"
antialias <- "subpixel"}}), print = FALSE)
# Get current path
pwd = dirname(rstudioapi::getActiveDocumentContext()$path)
pathSplit=strsplit(pwd, "/")
pathSplit=pathSplit[[1]]
main_path=paste0(pathSplit[1:(length(pathSplit)-1)],"/",collapse="")
analysis_path=paste0(pwd,"/")
path=paste0(main_path,"/Output/")
models_path = paste0(pwd,'/StanModels')
setwd(pwd)
# Valid participants (n = 59):
subjects = c(101:115,117:118,121:128,131:162,164:165)
# Load data ----
# CAT
filelist=c()
for (s in subjects){
filelist=c(filelist,Sys.glob(paste(path, "BM_",s,"_CAT_*.txt",sep="")))
}
CAT_data=c()
for (f in filelist){
CAT_data=rbind(CAT_data,read.table(f,header=T,na.strings=c(999,999000)))
}
# Probe
filelist=c()
for (s in subjects){
filelist=c(filelist,Sys.glob(paste(path, "BM_",s,"_Probe_*.txt",sep="")))
}
Probe_data=c()
for (f in filelist){
Probe_data=rbind(Probe_data,read.table(f,header=T,na.strings=c(999,999000)))
}
Probe_data$sub_i = as.numeric(factor(Probe_data$subjectID))
CAT_data$sub_i = as.numeric(factor(CAT_data$Subject))
CAT_data$Contingency2 = factor(CAT_data$Contingency, labels =  c("0% Contingency", "50% Contingency","100% Contingency"))
CAT_data$Run2 = as.factor(CAT_data$Run)
CAT_data$Response = 1 - is.na(CAT_data$RT)
CAT_data$RT_eff = CAT_data$RT - CAT_data$CueOnset
# Dataframe of Go trials with response
CAT_data_GO = subset(CAT_data, Go==1 & Response == 1 & RT>=250)
# STAN model - individual items - only 100%  contingency: data organization ====
stan_file = "training_mixture_individual_items_100Contingency.stan"
subs2test = 1:100 # set to smaller number to examine less participants for speed
dat = subset(CAT_data_GO, (sub_i %in% subs2test) & Contingency == 1)
N_trials_per_sub = dat %>% group_by(sub_i) %>% summarise(n()) %>% pull
N_trials_max = max(N_trials_per_sub)
N_runs = max(dat$Run)
N_subjects = nlevels(factor(dat$sub_i))
N_stim = dat %>% filter(sub_i ==1, Contingency==1) %>% pull(StimNum) %>% as.factor() %>% nlevels()
RT_table = matrix(data = 999000,nrow = N_trials_max,ncol = N_subjects)
Run_table = RT_table
Stim_table = RT_table
N_trials_valid = c()
for (n in 1:N_subjects){
# find values
RT_tmp = dat$RT[dat$sub_i == n]
Run_tmp = dat$Run[dat$sub_i == n]
Stim_tmp = dat$StimNum[dat$sub_i == n]
# scale values
Run_tmp = (Run_tmp-min(Run_tmp))/max(Run_tmp-min(Run_tmp))
Stim_tmp = as.numeric(as.factor(Stim_tmp))
# organize together
N_trials_valid[n] = sum(dat$sub_i == n)
RT_table[1:N_trials_valid[n],n] = RT_tmp
Run_table[1:N_trials_valid[n],n] = Run_tmp
Stim_table[1:N_trials_valid[n],n] = Stim_tmp
}
stan_data = list(
Run = Run_table,
Cue = 850,
theta_b0 = -3.1, # fix the proportion of early onset at pnorm(theta_b0) at the first run
RT = RT_table,
Stim = Stim_table,
N_trials = N_trials_max,
N_trials_valid = N_trials_valid,
N_runs = N_runs,
N_subjects = N_subjects,
N_stim = N_stim
)
write("// Stan model mixture gaus model - individual items - only 100% contingency
data {
int N_subjects;
int N_trials;
int N_trials_valid[N_subjects];
int N_stim;
real Cue;
real theta_b0;
real RT[N_trials, N_subjects];
real Run[N_trials, N_subjects];
int Stim[N_trials, N_subjects];
}
parameters {
// Fixed effect: mu1(Anticipatory), mu2(Cue dependent), theta1(100% contingency), theta2(50% contingency), sigma_e1, sigma_e2
real<lower=0, upper=Cue+100> mu1_fix; // Anticipatory mean
real<lower=Cue> mu2_fix; // Cue dependent mean
real theta_fix[1]; // theta0 + slope (for run)
real<lower=0> sigma_e[2]; // error terms
// SD for random effect: mu1, theta1, theta2
// real<lower=0> mu1_sd[1]; // SD of the random effects around the fixed mean
real<lower=0> theta_sub_sd[1]; // SD of the random effects around the fixed thetas
real<lower=0> theta_stim_sd[1]; // SD of the random effects around the fixed thetas
// Random effect parameter: mu1, theta1, theta2
// real <upper=Cue+100> mu1_rand[N_subjects]; // random intercept for early onest Gaussian's mean
// real theta_b0_rand[N_subjects]; // random intercept for theta - fix at -2.5
real theta_1_sub[N_subjects]; // random intercept for theta
// real theta_2_sub[N_subjects]; // random intercept for theta
real theta_1_sub_stim[N_subjects,N_stim]; // random intercept for theta
// real theta_2_sub_stim[N_subjects,N_stim]; // random intercept for theta
}
model {
//priors
mu1_fix ~ normal(500,500);
mu2_fix ~ normal(1000,500);
theta_fix ~ normal(0,0.7);
sigma_e ~ normal(0,1000);
theta_sub_sd ~ cauchy(0,1);
theta_stim_sd ~ cauchy(0,1);
// mu1_sd ~ normal(0,150);
//likelihood
theta_1_sub ~ normal(theta_fix[1],theta_sub_sd[1]); // 100% contingency
for (s in 1:N_subjects){
// mu1_rand[s] ~ normal(mu1_fix,mu1_sd);
// theta_b0_rand[s] ~ normal(theta_fix[1],theta_sub[1]); fix at -2.5
theta_1_sub_stim[s,] ~ normal(theta_1_sub[s],theta_stim_sd[1]); // 100% contingency
for (t in 1:N_trials_valid[s]){
int stim_i;
stim_i = Stim[t,s];
target += log_mix(Phi_approx(theta_b0 + theta_1_sub_stim[s,stim_i]*Run[t,s]),
normal_lpdf(RT[t,s] | mu1_fix, sigma_e[1]),
normal_lpdf(RT[t,s] | mu2_fix, sigma_e[2]));
}
}
}",
stan_file)
load(file = paste0(analysis_path,"fit_stan3.Rda"))
print(fit_stan3)
print(fit_stan3)
traceplot(fit_stan3, inc_warmup = FALSE)
# STAN model - individual items - only 50%  contingency: data organization ====
stan_file = "training_mixture_individual_items_50Contingency.stan"
subs2test = 1:100 # set to smaller number to examine less participants for speed
dat = subset(CAT_data_GO, (sub_i %in% subs2test) & Contingency == 0.5)
N_trials_per_sub = dat %>% group_by(sub_i) %>% summarise(n()) %>% pull
N_trials_max = max(N_trials_per_sub)
N_runs = max(dat$Run)
N_subjects = nlevels(factor(dat$sub_i))
N_stim = dat %>% filter(sub_i ==1, Contingency==0.5) %>% pull(StimNum) %>% as.factor() %>% nlevels()
RT_table = matrix(data = 999000,nrow = N_trials_max,ncol = N_subjects)
Run_table = RT_table
Stim_table = RT_table
N_trials_valid = c()
for (n in 1:N_subjects){
# find values
RT_tmp = dat$RT[dat$sub_i == n]
Run_tmp = dat$Run[dat$sub_i == n]
Stim_tmp = dat$StimNum[dat$sub_i == n]
# scale values
Run_tmp = (Run_tmp-min(Run_tmp))/max(Run_tmp-min(Run_tmp))
Stim_tmp = as.numeric(as.factor(Stim_tmp))
# organize together
N_trials_valid[n] = sum(dat$sub_i == n)
RT_table[1:N_trials_valid[n],n] = RT_tmp
Run_table[1:N_trials_valid[n],n] = Run_tmp
Stim_table[1:N_trials_valid[n],n] = Stim_tmp
}
stan_data = list(
Run = Run_table,
Cue = mean(dat$CueOnset),
theta_b0 = -3.1, # fix the proportion of early onset at pnorm(theta_b0) at the first run
RT = RT_table,
Stim = Stim_table,
N_trials = N_trials_max,
N_trials_valid = N_trials_valid,
N_runs = N_runs,
N_subjects = N_subjects,
N_stim = N_stim
)
write("// Stan model mixture gaus model - individual items - only 50% contingency
data {
int N_subjects;
int N_trials;
int N_trials_valid[N_subjects];
int N_stim;
real Cue;
real theta_b0;
real RT[N_trials, N_subjects];
real Run[N_trials, N_subjects];
int Stim[N_trials, N_subjects];
}
parameters {
// Fixed effect: mu1(Anticipatory), mu2(Cue dependent), theta1(100% contingency), theta2(50% contingency), sigma_e1, sigma_e2
real<lower=0, upper=Cue+100> mu1_fix; // Anticipatory mean
real<lower=Cue> mu2_fix; // Cue dependent mean
real theta_fix[1]; // theta0 + slope (for run)
real<lower=0> sigma_e[2]; // error terms
// SD for random effect: mu1, theta1, theta2
// real<lower=0> mu1_sd[1]; // SD of the random effects around the fixed mean
real<lower=0> theta_sub_sd[1]; // SD of the random effects around the fixed thetas
real<lower=0> theta_stim_sd[1]; // SD of the random effects around the fixed thetas
// Random effect parameter: mu1, theta1, theta2
// real <upper=Cue+100> mu1_rand[N_subjects]; // random intercept for early onest Gaussian's mean
// real theta_b0_rand[N_subjects]; // random intercept for theta - fix at -2.5
real theta_1_sub[N_subjects]; // random intercept for theta
// real theta_2_sub[N_subjects]; // random intercept for theta
real theta_1_sub_stim[N_subjects,N_stim]; // random intercept for theta
// real theta_2_sub_stim[N_subjects,N_stim]; // random intercept for theta
}
model {
//priors
mu1_fix ~ normal(500,500);
mu2_fix ~ normal(1000,500);
theta_fix ~ normal(0,0.7);
sigma_e ~ normal(0,1000);
theta_sub_sd ~ cauchy(0,1);
theta_stim_sd ~ cauchy(0,1);
// mu1_sd ~ normal(0,150);
//likelihood
theta_1_sub ~ normal(theta_fix[1],theta_sub_sd[1]); // 100% contingency
for (s in 1:N_subjects){
// mu1_rand[s] ~ normal(mu1_fix,mu1_sd);
// theta_b0_rand[s] ~ normal(theta_fix[1],theta_sub[1]); fix at -2.5
theta_1_sub_stim[s,] ~ normal(theta_1_sub[s],theta_stim_sd[1]); // 100% contingency
for (t in 1:N_trials_valid[s]){
int stim_i;
stim_i = Stim[t,s];
target += log_mix(Phi_approx(theta_b0 + theta_1_sub_stim[s,stim_i]*Run[t,s]),
normal_lpdf(RT[t,s] | mu1_fix, sigma_e[1]),
normal_lpdf(RT[t,s] | mu2_fix, sigma_e[2]));
}
}
}",
stan_file)
library(lme4)
library("rstudioapi")
library(rstan)
library(reshape2)
rm(list=ls())
options(mc.cores = parallel::detectCores())
# Get current path
script_path = rstudioapi::getActiveDocumentContext()$path
pathSplit=strsplit(script_path, "/")
pathSplit=pathSplit[[1]]
main_path=paste0(pathSplit[1:(length(pathSplit)-2)],"/",collapse="")
analysis_path=paste0(pathSplit[1:(length(pathSplit)-1)],"/",collapse="")
setwd(analysis_path)
## Sample
path=paste0(main_path,"/Output/")
subjects=c(101:110,112:121); # valid subjects
filelist=c()
for (s in subjects){
filelist=c(filelist,Sys.glob(paste(path, "BM_",s,"_CAT_*.txt",sep="")))
}
CAT_data=c()
for (f in filelist){
CAT_data=rbind(CAT_data,read.table(f,header=T,na.strings=c(999,999000)))
}
CAT_data$sub_i = as.numeric(factor(CAT_data$Subject))
CAT_data$Contingency2 = factor(CAT_data$Contingency, labels =  c("0% Contingency", "50% Contingency","100% Contingency"))
CAT_data$Run2 = as.factor(CAT_data$Run)
CAT_data$Response = 1 - is.na(CAT_data$RT)
CAT_data$RT_eff = CAT_data$RT - CAT_data$CueOnset
CAT_data_GO = subset(CAT_data, Go==1 & Response == 1 & RT>=250)
ggplot(data = CAT_data_GO, aes(x = Run, y = RT, color = Contingency2)) +
geom_point(position = 'jitter', alpha = 0.3) +
geom_smooth(method = 'lm')
# STAN model - individual items - only 50%  contingency: data organization ====
stan_file = "training_mixture_individual_items_50Contingency.stan"
subs2test = 1:100 # set to smaller number to examine less participants for speed
dat = subset(CAT_data_GO, (sub_i %in% subs2test) & Contingency == 0.5)
N_trials_per_sub = dat %>% group_by(sub_i) %>% summarise(n()) %>% pull
N_trials_max = max(N_trials_per_sub)
N_runs = max(dat$Run)
N_subjects = nlevels(factor(dat$sub_i))
N_stim = dat %>% filter(sub_i ==1, Contingency==0.5) %>% pull(StimNum) %>% as.factor() %>% nlevels()
RT_table = matrix(data = 999000,nrow = N_trials_max,ncol = N_subjects)
Run_table = RT_table
Stim_table = RT_table
N_trials_valid = c()
for (n in 1:N_subjects){
# find values
RT_tmp = dat$RT[dat$sub_i == n]
Run_tmp = dat$Run[dat$sub_i == n]
Stim_tmp = dat$StimNum[dat$sub_i == n]
# scale values
Run_tmp = (Run_tmp-min(Run_tmp))/max(Run_tmp-min(Run_tmp))
Stim_tmp = as.numeric(as.factor(Stim_tmp))
# organize together
N_trials_valid[n] = sum(dat$sub_i == n)
RT_table[1:N_trials_valid[n],n] = RT_tmp
Run_table[1:N_trials_valid[n],n] = Run_tmp
Stim_table[1:N_trials_valid[n],n] = Stim_tmp
}
stan_data = list(
Run = Run_table,
Cue = 850,
theta_b0 = -3.1, # fix the proportion of early onset at pnorm(theta_b0) at the first run
RT = RT_table,
Stim = Stim_table,
N_trials = N_trials_max,
N_trials_valid = N_trials_valid,
N_runs = N_runs,
N_subjects = N_subjects,
N_stim = N_stim
)
fit_stan4 <- stan(file = stan_file, data = stan_data, iter = 4000, chains = 4, seed = 1, control = list(adapt_delta =.85, max_treedepth=14))
stan_file
write("// Stan model mixture gaus model - individual items - only 50% contingency
data {
int N_subjects;
int N_trials;
int N_trials_valid[N_subjects];
int N_stim;
real Cue;
real theta_b0;
real RT[N_trials, N_subjects];
real Run[N_trials, N_subjects];
int Stim[N_trials, N_subjects];
}
parameters {
// Fixed effect: mu1(Anticipatory), mu2(Cue dependent), theta1(100% contingency), theta2(50% contingency), sigma_e1, sigma_e2
real<lower=0, upper=Cue+100> mu1_fix; // Anticipatory mean
real<lower=Cue> mu2_fix; // Cue dependent mean
real theta_fix[1]; // theta0 + slope (for run)
real<lower=0> sigma_e[2]; // error terms
// SD for random effect: mu1, theta1, theta2
// real<lower=0> mu1_sd[1]; // SD of the random effects around the fixed mean
real<lower=0> theta_sub_sd[1]; // SD of the random effects around the fixed thetas
real<lower=0> theta_stim_sd[1]; // SD of the random effects around the fixed thetas
// Random effect parameter: mu1, theta1, theta2
// real <upper=Cue+100> mu1_rand[N_subjects]; // random intercept for early onest Gaussian's mean
// real theta_b0_rand[N_subjects]; // random intercept for theta - fix at -2.5
real theta_1_sub[N_subjects]; // random intercept for theta
// real theta_2_sub[N_subjects]; // random intercept for theta
real theta_1_sub_stim[N_subjects,N_stim]; // random intercept for theta
// real theta_2_sub_stim[N_subjects,N_stim]; // random intercept for theta
}
model {
//priors
mu1_fix ~ normal(500,500);
mu2_fix ~ normal(1000,500);
theta_fix ~ normal(0,0.7);
sigma_e ~ normal(0,1000);
theta_sub_sd ~ cauchy(0,1);
theta_stim_sd ~ cauchy(0,1);
// mu1_sd ~ normal(0,150);
//likelihood
theta_1_sub ~ normal(theta_fix[1],theta_sub_sd[1]); // 100% contingency
for (s in 1:N_subjects){
// mu1_rand[s] ~ normal(mu1_fix,mu1_sd);
// theta_b0_rand[s] ~ normal(theta_fix[1],theta_sub[1]); fix at -2.5
theta_1_sub_stim[s,] ~ normal(theta_1_sub[s],theta_stim_sd[1]); // 100% contingency
for (t in 1:N_trials_valid[s]){
int stim_i;
stim_i = Stim[t,s];
target += log_mix(Phi_approx(theta_b0 + theta_1_sub_stim[s,stim_i]*Run[t,s]),
normal_lpdf(RT[t,s] | mu1_fix, sigma_e[1]),
normal_lpdf(RT[t,s] | mu2_fix, sigma_e[2]));
}
}
}",
stan_file)
fit_stan4 <- stan(file = stan_file, data = stan_data, iter = 4000, chains = 4, seed = 1, control = list(adapt_delta =.85, max_treedepth=14))
print(fit_stan4)
save(fit_stan4, file = paste0(analysis_path,"fit_stan4.Rda"))
analysis_path
# STAN model - individual items - only 100%  contingency: data organization ====
stan_file = "training_mixture_individual_items_100Contingency.stan"
subs2test = 1:100 # set to smaller number to examine less participants for speed
dat = subset(CAT_data_GO, (sub_i %in% subs2test) & Contingency == 1)
N_trials_per_sub = dat %>% group_by(sub_i) %>% summarise(n()) %>% pull
N_trials_max = max(N_trials_per_sub)
N_runs = max(dat$Run)
N_subjects = nlevels(factor(dat$sub_i))
N_stim = dat %>% filter(sub_i ==1, Contingency==1) %>% pull(StimNum) %>% as.factor() %>% nlevels()
RT_table = matrix(data = 999000,nrow = N_trials_max,ncol = N_subjects)
# STAN model - individual items - only 50%  contingency: data organization ====
stan_file = "training_mixture_individual_items_50Contingency.stan"
subs2test = 1:100 # set to smaller number to examine less participants for speed
dat = subset(CAT_data_GO, (sub_i %in% subs2test) & Contingency == 0.5)
N_trials_per_sub = dat %>% group_by(sub_i) %>% summarise(n()) %>% pull
N_trials_max = max(N_trials_per_sub)
N_runs = max(dat$Run)
N_subjects = nlevels(factor(dat$sub_i))
N_stim = dat %>% filter(sub_i ==1, Contingency==0.5) %>% pull(StimNum) %>% as.factor() %>% nlevels()
RT_table = matrix(data = 999000,nrow = N_trials_max,ncol = N_subjects)
Run_table = RT_table
Stim_table = RT_table
N_trials_valid = c()
for (n in 1:N_subjects){
# find values
RT_tmp = dat$RT[dat$sub_i == n]
Run_tmp = dat$Run[dat$sub_i == n]
Stim_tmp = dat$StimNum[dat$sub_i == n]
# scale values
Run_tmp = (Run_tmp-min(Run_tmp))/max(Run_tmp-min(Run_tmp))
Stim_tmp = as.numeric(as.factor(Stim_tmp))
# organize together
N_trials_valid[n] = sum(dat$sub_i == n)
RT_table[1:N_trials_valid[n],n] = RT_tmp
Run_table[1:N_trials_valid[n],n] = Run_tmp
Stim_table[1:N_trials_valid[n],n] = Stim_tmp
}
stan_data = list(
Run = Run_table,
Cue = 850,
theta_b0 = -3.1, # fix the proportion of early onset at pnorm(theta_b0) at the first run
RT = RT_table,
Stim = Stim_table,
N_trials = N_trials_max,
N_trials_valid = N_trials_valid,
N_runs = N_runs,
N_subjects = N_subjects,
N_stim = N_stim
)
# fit_stan4 <- stan(file = stan_file, data = stan_data, iter = 4000, chains = 4, seed = 1, control = list(adapt_delta =.85, max_treedepth=14))
# save(list = c("fit_stan4","stan_data"), file = paste0(analysis_path,"fit_stan4.Rda"))
load(file = paste0(analysis_path,"fit_stan4.Rda"))
save(list = c("fit_stan4","stan_data"), file = paste0(analysis_path,"fit_stan4.Rda"))
# STAN model - individual items - only 100%  contingency: data organization ====
stan_file = "training_mixture_individual_items_100Contingency.stan"
subs2test = 1:100 # set to smaller number to examine less participants for speed
dat = subset(CAT_data_GO, (sub_i %in% subs2test) & Contingency == 1)
N_trials_per_sub = dat %>% group_by(sub_i) %>% summarise(n()) %>% pull
N_trials_max = max(N_trials_per_sub)
N_runs = max(dat$Run)
N_subjects = nlevels(factor(dat$sub_i))
N_stim = dat %>% filter(sub_i ==1, Contingency==1) %>% pull(StimNum) %>% as.factor() %>% nlevels()
RT_table = matrix(data = 999000,nrow = N_trials_max,ncol = N_subjects)
Run_table = RT_table
Stim_table = RT_table
N_trials_valid = c()
for (n in 1:N_subjects){
# find values
RT_tmp = dat$RT[dat$sub_i == n]
Run_tmp = dat$Run[dat$sub_i == n]
Stim_tmp = dat$StimNum[dat$sub_i == n]
# scale values
Run_tmp = (Run_tmp-min(Run_tmp))/max(Run_tmp-min(Run_tmp))
Stim_tmp = as.numeric(as.factor(Stim_tmp))
# organize together
N_trials_valid[n] = sum(dat$sub_i == n)
RT_table[1:N_trials_valid[n],n] = RT_tmp
Run_table[1:N_trials_valid[n],n] = Run_tmp
Stim_table[1:N_trials_valid[n],n] = Stim_tmp
}
stan_data = list(
Run = Run_table,
Cue = 850,
theta_b0 = -3.1, # fix the proportion of early onset at pnorm(theta_b0) at the first run
RT = RT_table,
Stim = Stim_table,
N_trials = N_trials_max,
N_trials_valid = N_trials_valid,
N_runs = N_runs,
N_subjects = N_subjects,
N_stim = N_stim
)
fit_stan3 <- stan(file = stan_file, data = stan_data, iter = 4000, chains = 4, seed = 1, control = list(adapt_delta =.85, max_treedepth=14))
save(list = ("fit_stan3","stan_data"), file = paste0(analysis_path,"fit_stan3.Rda"))
save(list = c("fit_stan3","stan_data"), file = paste0(analysis_path,"fit_stan3.Rda"))
print(fit_stan3)
